{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        self.mapping = {'C': 0, 'H': 1, 'E': 2}\n",
    "\n",
    "    def one_hot_encode(self, sequence):\n",
    "        # Create a tensor for the sequence where each amino acid is replaced by its index\n",
    "        indices = torch.tensor([self.amino_acids.index(aa) for aa in sequence], dtype=torch.long)\n",
    "        # One-hot encode\n",
    "        return torch.nn.functional.one_hot(indices, num_classes=len(self.amino_acids)).type(torch.float)\n",
    "\n",
    "    def process_labels(self, label):\n",
    "        # Convert labels to indices\n",
    "        return torch.tensor([self.mapping[char] for char in label], dtype=torch.long)\n",
    "\n",
    "    def preprocess_data(self, filepath, num_sequences=None):\n",
    "        data = pd.read_csv(filepath)\n",
    "        filtered_data = data[data['has_nonstd_aa'] == False]\n",
    "\n",
    "        if num_sequences:\n",
    "            filtered_data = filtered_data[:num_sequences]\n",
    "\n",
    "        # Initialize lists for sequences and labels\n",
    "        sequence_tensors = [self.one_hot_encode(seq) for seq in filtered_data['seq']]\n",
    "        label_tensors = [self.process_labels(lbl) for lbl in filtered_data['sst3']]\n",
    "\n",
    "        # Padding sequences and labels\n",
    "        seq_lengths = torch.tensor([len(seq) for seq in sequence_tensors])\n",
    "        seq_tensor = torch.nn.utils.rnn.pad_sequence(sequence_tensors, batch_first=True, padding_value=0)\n",
    "        label_tensor = torch.nn.utils.rnn.pad_sequence(label_tensors, batch_first=True, padding_value=-1)  # Use -1 for label padding\n",
    "\n",
    "        # Sort by sequence length in descending order for pack_padded_sequence\n",
    "        lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "        seq_tensor = seq_tensor[perm_idx]\n",
    "        label_tensor = label_tensor[perm_idx]\n",
    "        return seq_tensor, label_tensor, lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        #the problem problem lies in the lstm or in the pad_packed_sequence, after that there is a dimension mismatch\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x,lengths):\n",
    "        print(f'before padcking {x.size()}')\n",
    "        packed_x = pack_padded_sequence(x,lengths, batch_first=True, enforce_sorted=True)\n",
    "        packed_output, (hn,cn) = self.lstm(packed_x)\n",
    "        assert packed_x.data.size(0) == packed_output.data.size(0)\n",
    "        #this is still true, no assertion error\n",
    "        #so problem must lie in pad_packed_sequence\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        print(f\"after padding:{output.size()}\")\n",
    "        out = self.fc(output)\n",
    "        return out\n",
    "\n",
    "        \n",
    "\n",
    "class AminoAcidDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, lengths):\n",
    "        self.sequences = sequences \n",
    "        self.lengths = lengths # A list of encoded amino acid sequences (as tensors)\n",
    "        self.labels = labels        # Corresponding labels or targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set num_sequences to False if you want all of them\n",
    "preprocessor = Preprocessing()\n",
    "sequence_tensors, label_tensors, lengths_tensor = preprocessor.preprocess_data(\"2018-06-06-ss.cleaned.csv\", num_sequences= 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train,validation,test\n",
    "total = len(sequence_tensors)\n",
    "train_size = int(0.8 * total)\n",
    "val_size = int(0.2 * total)\n",
    "\n",
    "\n",
    "indices = torch.randperm(len(sequence_tensors)).tolist()\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_sequences = sequence_tensors[:train_size]\n",
    "train_labels = label_tensors[:train_size]\n",
    "train_lengths = lengths_tensor[:train_size]\n",
    "\n",
    "test_sequences = sequence_tensors[train_size:]\n",
    "test_labels = label_tensors[train_size:]\n",
    "test_lengths = lengths_tensor[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#lengths = torch.tensor([len(seq) for seq in sequence_tensors])\n",
    "dataset = AminoAcidDataset(sequence_tensors, label_tensors, lengths_tensor)\n",
    "loader = DataLoader(dataset, batch_size = 1000, shuffle = False)\n",
    "train_dataset = AminoAcidDataset(train_sequences,train_labels, train_lengths)\n",
    "val_dataset = AminoAcidDataset(test_sequences, test_labels, test_lengths)\n",
    "trainloader = DataLoader(train_dataset, batch_size = 100, shuffle = True)\n",
    "valloader = DataLoader(val_dataset, batch_size = 100, shuffle = True)\n",
    "#pack_padded_sequence(padded_sequences,...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model, loss function and optimizer\n",
    "\n",
    "model = Model(input_dim = 20, hidden_dim =64 , layer_dim = 1, output_dim = 3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before padcking torch.Size([1000, 9, 20])\n",
      "after padding:torch.Size([1000, 9, 64])\n",
      "Expected output size: [batch_size, max_seq_length, num_classes] -> 1000, 9, 3\n",
      "Actual output size: torch.Size([1000, 9, 3])\n",
      "Labels size: torch.Size([1000, 9])\n",
      "torch.Size([1000, 9, 3])\n",
      "torch.Size([1000, 9])\n",
      "1\n",
      "before padcking torch.Size([1000, 9, 20])\n",
      "after padding:torch.Size([1000, 7, 64])\n",
      "Expected output size: [batch_size, max_seq_length, num_classes] -> 1000, 7, 3\n",
      "Actual output size: torch.Size([1000, 7, 3])\n",
      "Labels size: torch.Size([1000, 9])\n",
      "torch.Size([1000, 7, 3])\n",
      "torch.Size([1000, 9])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [9000] at index 0 does not match the shape of the indexed tensor [7000, 3] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#since we have used -1 for padding for the labels and cross_entropy can\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#not handle -1, we ignore those in our loss calculation\u001b[39;00m\n\u001b[1;32m     27\u001b[0m mask \u001b[38;5;241m=\u001b[39m labels_flat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m outputs_flat \u001b[38;5;241m=\u001b[39m \u001b[43moutputs_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m labels_flat \u001b[38;5;241m=\u001b[39m labels_flat[mask]\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs_flat, labels_flat)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [9000] at index 0 does not match the shape of the indexed tensor [7000, 3] at index 0"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# Initialize lists to monitor loss\n",
    "train_losses = []\n",
    "\n",
    "# Assuming you have a validation dataset and loader\n",
    "# val_loader = ...\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for sequences, labels, lengthss in loader:\n",
    "        count =0\n",
    "        optimizer.zero_grad()\n",
    "        sequences = sequences.float()\n",
    "        \n",
    "        outputs = model(sequences, lengthss)\n",
    "        print(f\"Expected output size: [batch_size, max_seq_length, num_classes] -> {labels.size(0)}, {torch.max(lengthss)}, {outputs.size(2)}\")\n",
    "        print(f\"Actual output size: {outputs.size()}\")\n",
    "        print(f\"Labels size: {labels.size()}\")\n",
    "        print(outputs.size())\n",
    "        print(labels.size())\n",
    "        outputs_flat = outputs.view(-1, 3)\n",
    "        labels_flat = labels.view(-1)\n",
    "\n",
    "        #since we have used -1 for padding for the labels and cross_entropy can\n",
    "        #not handle -1, we ignore those in our loss calculation\n",
    "        mask = labels_flat != -1\n",
    "        outputs_flat = outputs_flat[mask]\n",
    "        labels_flat = labels_flat[mask]\n",
    "        loss = criterion(outputs_flat, labels_flat)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * sequences.size(0)\n",
    "        count+=1\n",
    "        print(count)\n",
    "        \n",
    "    # Average loss for this epoch\n",
    "    epoch_loss /= len(trainloader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for sequences, lengths, labels in valloader:\n",
    "            # Similar steps to compute validation loss\n",
    "            # Update val_loss\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
