{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Secondary Structure Prediction\n",
    "\n",
    "## Part 1: Deep Learning with LSTM Networks\n",
    "\n",
    "This notebook implements protein secondary structure prediction using two approaches:\n",
    "1. **LSTM (Long Short-Term Memory) Networks** - Deep Learning approach\n",
    "2. **Hidden Markov Models (HMM)** - Probabilistic approach with Baum-Welch algorithm\n",
    "\n",
    "Author: Michele Copetti  \n",
    "Course: Interdisciplinary Research Methods in Computational Biology  \n",
    "Date: June 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from itertools import groupby\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for LSTM\n",
    "\n",
    "The preprocessing class handles:\n",
    "- Filtering sequences without non-standard amino acids\n",
    "- Parsing sequences to extract segments of target length\n",
    "- One-hot encoding amino acids for LSTM input\n",
    "- Converting labels to categorical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline for protein sequence data.\n",
    "    \n",
    "    Attributes:\n",
    "        amino_acids (str): Standard 20 amino acids\n",
    "        mapping_sst3 (dict): Mapping for 3-class secondary structure\n",
    "        target_length (int): Fixed length for sequence segments\n",
    "        secondary_type (str): Type of secondary structure classification ('sst3' or 'sst8')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_length, secondary_type='sst3'):\n",
    "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        self.mapping_sst3 = {'C': 0, 'H': 1, 'E': 2}  # Other: 0, Helix: 1, Sheet: 2\n",
    "        self.mapping_sst8 = {'C': 0, 'H': 1, 'E': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
    "        self.target_length = target_length\n",
    "        self.secondary_type = secondary_type\n",
    "        self.sequences = None\n",
    "        self.labels = None\n",
    "\n",
    "    def one_hot_encode(self, sequence):\n",
    "        \"\"\"\n",
    "        Convert amino acid sequence to one-hot encoded tensor.\n",
    "        \n",
    "        Args:\n",
    "            sequence (str): Amino acid sequence\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: One-hot encoded representation (seq_len, 20)\n",
    "        \"\"\"\n",
    "        indices = torch.tensor([self.amino_acids.index(aa) for aa in sequence], dtype=torch.long)\n",
    "        return torch.nn.functional.one_hot(indices, num_classes=len(self.amino_acids)).type(torch.float)\n",
    "\n",
    "    def process_labels(self, label):\n",
    "        \"\"\"\n",
    "        Convert secondary structure labels to categorical indices.\n",
    "        \n",
    "        Args:\n",
    "            label (str): Secondary structure label string\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Label indices\n",
    "        \"\"\"\n",
    "        if self.secondary_type == 'sst3':\n",
    "            return torch.tensor([self.mapping_sst3[char] for char in label], dtype=torch.long)\n",
    "        elif self.secondary_type == 'sst8':\n",
    "            return torch.tensor([self.mapping_sst8[char] for char in label], dtype=torch.long)\n",
    "    \n",
    "    def parser(self, labels, sequences):\n",
    "        \"\"\"\n",
    "        Parse sequences to extract segments of target length with consistent labels.\n",
    "        \n",
    "        Args:\n",
    "            labels (str): Secondary structure label sequence\n",
    "            sequences (str): Amino acid sequence\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (list of sequence segments, list of corresponding labels)\n",
    "        \"\"\"\n",
    "        # Group consecutive identical labels\n",
    "        categories = [''.join(g) for _, g in groupby(labels)]\n",
    "        \n",
    "        splits = []\n",
    "        corresponding_labels = []\n",
    "        start = 0\n",
    "        \n",
    "        if categories:\n",
    "            for category in categories:\n",
    "                length = len(category)\n",
    "                split = sequences[start:start+length]\n",
    "                \n",
    "                # Only keep segments of exact target length\n",
    "                if len(split) == self.target_length:\n",
    "                    splits.append(split)\n",
    "                    corresponding_labels.append(category[0])  # Single label per segment\n",
    "                    break  # One segment per protein sequence\n",
    "                \n",
    "                start += length\n",
    "        \n",
    "        return splits, corresponding_labels\n",
    "\n",
    "    def preprocess_data(self, filepath, num_sequences=None):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline for protein sequence dataset.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to CSV dataset\n",
    "            num_sequences (int, optional): Number of sequences to process\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (sequence tensors, label tensors)\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(filepath)\n",
    "        \n",
    "        # Filter out sequences with non-standard amino acids\n",
    "        filtered_data = data[data['has_nonstd_aa'] == False]\n",
    "\n",
    "        if num_sequences:\n",
    "            filtered_data = filtered_data.sample(n=num_sequences, random_state=42)\n",
    "\n",
    "        # Parse sequences and labels\n",
    "        labels = []\n",
    "        sequences = []\n",
    "        \n",
    "        for lbl, seq in zip(filtered_data[self.secondary_type], filtered_data['seq']):\n",
    "            sequence, label = self.parser(lbl, seq)\n",
    "            if sequence:\n",
    "                sequences.extend(sequence)\n",
    "                labels.append(label[0])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_tensors = [self.one_hot_encode(seq) for seq in sequences]\n",
    "        sequence_tensors = torch.stack(sequence_tensors).squeeze(0)\n",
    "        label_tensors = self.process_labels(labels)\n",
    "\n",
    "        self.sequences = sequence_tensors\n",
    "        self.labels = label_tensors\n",
    "        \n",
    "        return sequence_tensors, label_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "**Important**: Update the `path` variable below with the location of your dataset.  \n",
    "Dataset: [Protein Secondary Structure Dataset](https://www.kaggle.com/datasets/alfrandom/protein-secondary-structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SECONDARY_OUTPUT_NUMBER = 3  # 3-class classification: Other, Helix, Sheet\n",
    "TARGET_LENGTH = 7  # Typical length for alpha helices and beta sheets\n",
    "NUM_SEQUENCES = 40000  # Number of sequences to process (set to None for all)\n",
    "\n",
    "# Update this path to your dataset location\n",
    "DATA_PATH = '/path/to/your/2018-06-06-ss.cleaned.csv'\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = Preprocessing(target_length=TARGET_LENGTH, secondary_type='sst3')\n",
    "\n",
    "# Preprocess data\n",
    "sequence_tensors, label_tensors = preprocessor.preprocess_data(DATA_PATH, num_sequences=NUM_SEQUENCES)\n",
    "\n",
    "print(f\"Sequence tensor shape: {sequence_tensors.shape}\")\n",
    "print(f\"Label tensor shape: {label_tensors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Definition\n",
    "\n",
    "Our LSTM model consists of:\n",
    "- Multiple stacked LSTM layers for hierarchical feature learning\n",
    "- A fully connected layer for final classification\n",
    "- Output dimension of 3 for three secondary structure classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based model for protein secondary structure prediction.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Input feature dimension (20 for amino acids)\n",
    "        hidden_dim (int): Hidden state dimension\n",
    "        layer_dim (int): Number of LSTM layers\n",
    "        output_dim (int): Output dimension (3 for 3-class classification)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layer: processes sequential input\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer: maps LSTM output to class probabilities\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequences (batch_size, seq_len, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Class logits (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Take the last time step output for classification\n",
    "        out = self.fc(output[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class AminoAcidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset wrapper for amino acid sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequences (torch.Tensor): Sequence data\n",
    "        labels (torch.Tensor): Corresponding labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split\n",
    "\n",
    "Split the dataset:\n",
    "- 60% for training\n",
    "- 20% for validation\n",
    "- 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split sizes\n",
    "total = len(sequence_tensors)\n",
    "train_size = int(0.6 * total)\n",
    "val_size = int(0.2 * total)\n",
    "test_size = total - train_size - val_size\n",
    "\n",
    "# Generate random permutation for splitting\n",
    "indices = torch.randperm(total).tolist()\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "# Create data splits\n",
    "train_sequences = sequence_tensors[train_indices]\n",
    "train_labels = label_tensors[train_indices]\n",
    "\n",
    "val_sequences = sequence_tensors[val_indices]\n",
    "val_labels = label_tensors[val_indices]\n",
    "\n",
    "test_sequences = sequence_tensors[test_indices]\n",
    "test_labels = label_tensors[test_indices]\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = AminoAcidDataset(train_sequences, train_labels)\n",
    "val_dataset = AminoAcidDataset(val_sequences, val_labels)\n",
    "test_dataset = AminoAcidDataset(test_sequences, test_labels)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Loss, and Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Model(\n",
    "    input_dim=20,  # 20 amino acids\n",
    "    hidden_dim=140,  # Hidden state size\n",
    "    layer_dim=6,  # Number of LSTM layers\n",
    "    output_dim=SECONDARY_OUTPUT_NUMBER  # 3 classes\n",
    ")\n",
    "\n",
    "# Loss function for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer with learning rate 10^-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the model for multiple epochs while monitoring both training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 175\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ==================== Training Phase ====================\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for sequences, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        sequences = sequences.float()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        outputs_flat = outputs.view(-1, SECONDARY_OUTPUT_NUMBER)\n",
    "        labels_flat = labels.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs_flat, labels_flat)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item() * sequences.size(0)\n",
    "        _, predicted = torch.max(outputs_flat, 1)\n",
    "        train_correct += (predicted == labels_flat).sum().item()\n",
    "        train_total += labels_flat.size(0)\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    epoch_loss /= len(trainloader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    \n",
    "    # ==================== Validation Phase ====================\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in valloader:\n",
    "            sequences = sequences.float()\n",
    "            outputs = model(sequences)\n",
    "            outputs_flat = outputs.view(-1, SECONDARY_OUTPUT_NUMBER)\n",
    "            labels_flat = labels.view(-1)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss += criterion(outputs_flat, labels_flat).item() * sequences.size(0)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs_flat, 1)\n",
    "            val_correct += (predicted == labels_flat).sum().item()\n",
    "            val_total += labels_flat.size(0)\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss /= len(valloader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}]')\n",
    "        print(f'  Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot losses\n",
    "sns.lineplot(x=range(len(train_losses)), y=train_losses, ax=ax1, \n",
    "             label='Training Loss', color='blue', linewidth=2)\n",
    "sns.lineplot(x=range(len(val_losses)), y=val_losses, ax=ax1, \n",
    "             label='Validation Loss', color='orange', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax1.set_title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plot validation accuracy\n",
    "sns.lineplot(x=range(len(val_accuracies)), y=val_accuracies, ax=ax2, \n",
    "             label='Validation Accuracy', color='green', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    "ax2.set_title('Validation Accuracy Over Epochs', fontsize=16)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in testloader:\n",
    "        outputs = model(sequences)\n",
    "        outputs_flat = outputs.view(-1, SECONDARY_OUTPUT_NUMBER)\n",
    "        labels_flat = labels.view(-1)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs_flat, 1)\n",
    "        test_correct += (predicted == labels_flat).sum().item()\n",
    "        test_total += labels_flat.size(0)\n",
    "        \n",
    "        # Store for classification report\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}\\n')\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "report = classification_report(\n",
    "    all_labels, \n",
    "    all_predictions, \n",
    "    target_names=['Other (C)', 'Helix (H)', 'Sheet (E)']\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Normalize confusion matrix\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_normalized, \n",
    "    annot=True, \n",
    "    fmt='.2f', \n",
    "    cmap=\"Blues\", \n",
    "    xticklabels=['C', 'H', 'E'], \n",
    "    yticklabels=['C', 'H', 'E']\n",
    ")\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.ylabel(\"True\", fontsize=12)\n",
    "plt.title(\"Normalized Confusion Matrix - LSTM Model\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Hidden Markov Model Approach\n",
    "\n",
    "This section implements protein secondary structure prediction using Hidden Markov Models (HMM) trained with the Baum-Welch algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for HMM\n",
    "\n",
    "HMMs use integer encoding instead of one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingHMM:\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline for HMM approach.\n",
    "    Uses integer encoding instead of one-hot encoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_length, secondary_type='sst3'):\n",
    "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        self.mapping_sst3 = {'C': 0, 'H': 1, 'E': 2}\n",
    "        self.mapping_sst8 = {'C': 0, 'H': 1, 'E': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
    "        self.target_length = target_length\n",
    "        self.secondary_type = secondary_type\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        \"\"\"Convert amino acid sequence to integer indices.\"\"\"\n",
    "        return np.array([self.amino_acids.index(aa) for aa in sequence], dtype=int)\n",
    "\n",
    "    def process_labels(self, label):\n",
    "        \"\"\"Convert labels to integer indices.\"\"\"\n",
    "        if self.secondary_type == 'sst3':\n",
    "            return np.array([self.mapping_sst3[char] for char in label])\n",
    "        elif self.secondary_type == 'sst8':\n",
    "            return np.array([self.mapping_sst8[char] for char in label])\n",
    "    \n",
    "    def parser(self, labels, sequences):\n",
    "        \"\"\"Parse sequences and labels.\"\"\"\n",
    "        categories = [''.join(g) for _, g in groupby(labels)]\n",
    "        splits = []\n",
    "        corresponding_labels = []\n",
    "        start = 0\n",
    "        \n",
    "        for category in categories:\n",
    "            length = len(category)\n",
    "            split = sequences[start:start+length]\n",
    "            if len(split) == self.target_length:\n",
    "                splits.append(split)\n",
    "                corresponding_labels.append(category[0])\n",
    "                break\n",
    "            start += length\n",
    "            \n",
    "        return splits, corresponding_labels\n",
    "\n",
    "    def preprocess_data(self, filepath, num_sequences=None):\n",
    "        \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "        data = pd.read_csv(filepath)\n",
    "        filtered_data = data[data['has_nonstd_aa'] == False]\n",
    "\n",
    "        if num_sequences:\n",
    "            filtered_data = filtered_data.sample(n=num_sequences, random_state=42)\n",
    "\n",
    "        labels = []\n",
    "        sequences = []\n",
    "        \n",
    "        for lbl, seq in zip(filtered_data[self.secondary_type], filtered_data['seq']):\n",
    "            sequence, label = self.parser(lbl, seq)\n",
    "            if sequence:\n",
    "                sequences.extend(sequence)\n",
    "                labels.extend(label)\n",
    "\n",
    "        # Convert to arrays\n",
    "        sequence_arrays = [self.encode_sequence(seq) for seq in sequences]\n",
    "        label_arrays = self.process_labels(labels)\n",
    "        \n",
    "        return sequence_arrays, label_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data for HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for HMM\n",
    "preprocessor_hmm = PreprocessingHMM(target_length=7, secondary_type='sst3')\n",
    "sequences, labels = preprocessor_hmm.preprocess_data(DATA_PATH, num_sequences=50000)\n",
    "\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function: Split Sequences by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_class(sequences, labels):\n",
    "    \"\"\"\n",
    "    Split sequences into three lists based on their class labels.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): List of sequences\n",
    "        labels (list): Corresponding labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (helix_sequences, sheet_sequences, other_sequences)\n",
    "    \"\"\"\n",
    "    helix = []\n",
    "    sheet = []\n",
    "    other = []\n",
    "    \n",
    "    for sequence, label in zip(sequences, labels):\n",
    "        if label == 1:  # Helix\n",
    "            helix.append(sequence)\n",
    "        elif label == 2:  # Sheet\n",
    "            sheet.append(sequence)\n",
    "        else:  # Other\n",
    "            other.append(sequence)\n",
    "    \n",
    "    return helix, sheet, other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model Implementation\n",
    "\n",
    "Complete implementation of HMM with Baum-Welch training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \"\"\"\n",
    "    Hidden Markov Model with Baum-Welch algorithm for training.\n",
    "    \n",
    "    Args:\n",
    "        num_states (int): Number of hidden states\n",
    "        num_symbols (int): Number of observation symbols (20 amino acids)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_symbols):\n",
    "        self.num_states = num_states\n",
    "        self.num_symbols = num_symbols\n",
    "        self.epsilon = 1e-10  # Prevent log(0) errors\n",
    "\n",
    "        # Initialize probabilities randomly with small epsilon\n",
    "        self.transition_probs = np.random.rand(num_states, num_states) + self.epsilon\n",
    "        self.emission_probs = np.random.rand(num_states, num_symbols) + self.epsilon\n",
    "        self.initial_probs = np.random.rand(num_states) + self.epsilon\n",
    "\n",
    "        # Normalize to ensure probabilities sum to 1\n",
    "        self.transition_probs /= self.transition_probs.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs /= self.emission_probs.sum(axis=1, keepdims=True)\n",
    "        self.initial_probs /= self.initial_probs.sum()\n",
    "\n",
    "    def forward_algorithm(self, seq):\n",
    "        \"\"\"\n",
    "        Forward algorithm: compute probability of observations up to time t.\n",
    "        \n",
    "        Args:\n",
    "            seq (np.array): Observation sequence\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Forward probabilities (T, num_states)\n",
    "        \"\"\"\n",
    "        T = len(seq)\n",
    "        alpha = np.zeros((T, self.num_states))\n",
    "        \n",
    "        # Initialization\n",
    "        for i in range(self.num_states):\n",
    "            alpha[0][i] = self.initial_probs[i] * self.emission_probs[i][seq[0]]\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.num_states):\n",
    "                alpha[t][j] = np.sum(\n",
    "                    alpha[t-1] * self.transition_probs[:, j] * self.emission_probs[j][seq[t]]\n",
    "                )\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    def backward_algorithm(self, seq):\n",
    "        \"\"\"\n",
    "        Backward algorithm: compute probability from time t+1 to end.\n",
    "        \n",
    "        Args:\n",
    "            seq (np.array): Observation sequence\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Backward probabilities (T, num_states)\n",
    "        \"\"\"\n",
    "        T = len(seq)\n",
    "        beta = np.zeros((T, self.num_states))\n",
    "        \n",
    "        # Initialization\n",
    "        beta[T - 1] = 1\n",
    "\n",
    "        # Recursion (backward)\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for i in range(self.num_states):\n",
    "                beta[t][i] = np.sum(\n",
    "                    self.transition_probs[i] * \n",
    "                    self.emission_probs[:, seq[t+1]] * \n",
    "                    beta[t+1]\n",
    "                )\n",
    "        \n",
    "        return beta\n",
    "\n",
    "    def expectation_step(self, seq):\n",
    "        \"\"\"\n",
    "        E-step: Calculate expected state occupancy (gamma) and\n",
    "        expected state transitions (xi).\n",
    "        \n",
    "        Args:\n",
    "            seq (np.array): Observation sequence\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (xi, gamma)\n",
    "        \"\"\"\n",
    "        T = len(seq)\n",
    "        alpha = self.forward_algorithm(seq)\n",
    "        beta = self.backward_algorithm(seq)\n",
    "\n",
    "        # Calculate gamma: probability of being in state i at time t\n",
    "        gamma = alpha * beta\n",
    "        gamma /= gamma.sum(axis=1, keepdims=True)  # Normalize\n",
    "        \n",
    "        # Calculate xi: probability of transition from i to j at time t\n",
    "        xi = np.zeros((T - 1, self.num_states, self.num_states))\n",
    "        for t in range(T - 1):\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    xi[t][i][j] = (\n",
    "                        alpha[t][i] * \n",
    "                        self.transition_probs[i][j] * \n",
    "                        self.emission_probs[j][seq[t+1]] * \n",
    "                        beta[t+1][j]\n",
    "                    )\n",
    "            xi[t] /= xi[t].sum()  # Normalize\n",
    "        \n",
    "        return xi, gamma\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        \"\"\"Normalize all probability matrices to ensure they sum to 1.\"\"\"\n",
    "        self.transition_probs /= self.transition_probs.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs /= self.emission_probs.sum(axis=1, keepdims=True)\n",
    "        self.initial_probs /= self.initial_probs.sum()\n",
    "\n",
    "    def train(self, sequences, num_iterations=1000):\n",
    "        \"\"\"\n",
    "        Train HMM using Baum-Welch algorithm.\n",
    "        \n",
    "        Args:\n",
    "            sequences (list): List of observation sequences\n",
    "            num_iterations (int): Number of EM iterations\n",
    "        \"\"\"\n",
    "        R = len(sequences)  # Number of sequences\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            # Initialize accumulators for M-step\n",
    "            total_xi_probs = np.zeros_like(self.transition_probs)\n",
    "            gamma_indicator_probs = np.zeros_like(self.emission_probs)\n",
    "            total_gamma_probs = np.zeros((self.num_states, 1))\n",
    "            total_initial_probs = np.zeros_like(self.initial_probs)\n",
    "\n",
    "            # E-step: Calculate expectations for all sequences\n",
    "            for seq in sequences:\n",
    "                xi, gamma = self.expectation_step(seq)\n",
    "                \n",
    "                # Accumulate statistics\n",
    "                total_xi_probs += xi.sum(axis=0)\n",
    "                total_initial_probs += gamma[0]\n",
    "                total_gamma_probs += gamma[:-1].sum(axis=0).reshape(-1, 1)\n",
    "                \n",
    "                # Accumulate emission statistics\n",
    "                for t in range(len(seq)):\n",
    "                    symbol = seq[t]\n",
    "                    gamma_indicator_probs[:, symbol] += gamma[t]\n",
    "\n",
    "            # M-step: Update parameters\n",
    "            self.transition_probs = total_xi_probs / total_gamma_probs\n",
    "            self.emission_probs = gamma_indicator_probs / total_gamma_probs\n",
    "            self.initial_probs = total_initial_probs / R\n",
    "            \n",
    "            # Ensure probabilities sum to 1\n",
    "            self.normalize_parameters()\n",
    "            \n",
    "            # Print progress\n",
    "            if (iteration + 1) % 100 == 0:\n",
    "                print(f\"  Iteration {iteration + 1}/{num_iterations} complete\")\n",
    "\n",
    "    def score(self, seq):\n",
    "        \"\"\"\n",
    "        Calculate log-likelihood of sequence under this model.\n",
    "        \n",
    "        Args:\n",
    "            seq (np.array): Observation sequence\n",
    "            \n",
    "        Returns:\n",
    "            float: Log-likelihood\n",
    "        \"\"\"\n",
    "        alpha = self.forward_algorithm(seq)\n",
    "        return alpha[-1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate HMM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_hmm(sequences, labels, n_iter=1000):\n",
    "    \"\"\"\n",
    "    Train three HMMs (one per class) and evaluate on test set.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): List of sequences\n",
    "        labels (list): Corresponding labels\n",
    "        n_iter (int): Number of training iterations\n",
    "    \"\"\"\n",
    "    # Split data into train and test\n",
    "    training, testing, train_labels, test_labels = train_test_split(\n",
    "        sequences, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Split training data by class\n",
    "    helix, sheet, other = split_by_class(training, train_labels)\n",
    "    \n",
    "    print(f\"Training set sizes:\")\n",
    "    print(f\"  Helix: {len(helix)}\")\n",
    "    print(f\"  Sheet: {len(sheet)}\")\n",
    "    print(f\"  Other: {len(other)}\\n\")\n",
    "\n",
    "    # Create and train three HMMs\n",
    "    print(\"Training Helix HMM...\")\n",
    "    hmm_helix = HMM(num_states=3, num_symbols=20)\n",
    "    hmm_helix.train(helix, num_iterations=n_iter)\n",
    "    \n",
    "    print(\"\\nTraining Sheet HMM...\")\n",
    "    hmm_sheet = HMM(num_states=3, num_symbols=20)\n",
    "    hmm_sheet.train(sheet, num_iterations=n_iter)\n",
    "    \n",
    "    print(\"\\nTraining Other HMM...\")\n",
    "    hmm_other = HMM(num_states=3, num_symbols=20)\n",
    "    hmm_other.train(other, num_iterations=n_iter)\n",
    "\n",
    "    # Prediction\n",
    "    models = [hmm_other, hmm_helix, hmm_sheet]  # Order: 0, 1, 2\n",
    "    prediction_labels = [0, 1, 2]  # Other, Helix, Sheet\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    correct = 0\n",
    "    total = len(testing)\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    # Predict for each test sequence\n",
    "    for sequence, true_label in zip(testing, test_labels):\n",
    "        # Calculate likelihood under each model\n",
    "        log_probs = [model.score(sequence) for model in models]\n",
    "        \n",
    "        # Predict class with highest likelihood\n",
    "        best_model_idx = np.argmax(log_probs)\n",
    "        predicted_label = prediction_labels[best_model_idx]\n",
    "\n",
    "        all_true_labels.append(true_label)\n",
    "        all_pred_labels.append(predicted_label)\n",
    "\n",
    "        if true_label == predicted_label:\n",
    "            correct += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f'\\nAccuracy: {accuracy:.2f}%\\n')\n",
    "\n",
    "    # Print classification report\n",
    "    target_names = ['Other', 'Helix', 'Sheet']\n",
    "    print(\"Classification Report:\")\n",
    "    report = classification_report(all_true_labels, all_pred_labels, target_names=target_names)\n",
    "    print(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_matrix_normalized, \n",
    "        annot=True, \n",
    "        fmt='.2f', \n",
    "        cmap=\"Blues\", \n",
    "        xticklabels=target_names, \n",
    "        yticklabels=target_names\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.title(\"Normalized Confusion Matrix - HMM Model\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Train and evaluate\n",
    "train_and_evaluate_hmm(sequences, labels, n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated two approaches for protein secondary structure prediction:\n",
    "\n",
    "### LSTM Approach\n",
    "- **Accuracy**: ~88%\n",
    "- **Strengths**: High accuracy, captures long-range dependencies\n",
    "- **Limitations**: Black-box model, computationally expensive\n",
    "\n",
    "### HMM Approach\n",
    "- **Accuracy**: ~76%\n",
    "- **Strengths**: Interpretable, computationally efficient, theoretically grounded\n",
    "- **Limitations**: Lower accuracy, restrictive Markov assumptions\n",
    "\n",
    "### Key Takeaways\n",
    "1. Deep learning (LSTM) provides superior predictive accuracy\n",
    "2. HMMs offer interpretability and computational efficiency\n",
    "3. Both approaches are viable depending on the application requirements\n",
    "4. The choice depends on the trade-off between accuracy and interpretability\n",
    "\n",
    "For detailed analysis and discussion, please refer to the accompanying report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
